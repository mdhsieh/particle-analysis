# -*- coding: utf-8 -*-
"""mask-rcnn_cell_segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IvyFmQ2fdC_JAw6TjWIOn5xFJoO882F5

# cell segmentation by mask r-cnn
"""

# mount drive
from google.colab import drive
drive.mount('/content/drive')

"""# Install detectron2"""

# error from using colab default torch 1.9
# ImportError: /usr/local/lib/python3.7/dist-packages/detectron2/_C.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZNK2at6Tensor7is_cudaEv
# see: https://github.com/facebookresearch/detectron2/issues/3166
# pytorch must be installed with a specific torchvision version, see:
# https://detectron2.readthedocs.io/en/latest/tutorials/install.html
# https://github.com/pytorch/vision/
# need pytorch 1.8.0, corresponding torchvision 0.9.0, and CUDA 10.1
!pip install torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html

# install dependencies: 
!pip install pyyaml==5.1
import torch, torchvision
print(torch.__version__, torch.cuda.is_available())
!gcc --version
# opencv is pre-installed on colab

# install detectron2: (Colab has CUDA 10.1 + torch 1.8)
# See https://detectron2.readthedocs.io/tutorials/install.html for instructions
import torch
assert torch.__version__.startswith("1.8")   # need to manually install torch 1.8 if Colab changes its default version

!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html
# exit(0)  # After installation, you need to "restart runtime" in Colab. This line can also restart runtime

# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import os, json, cv2, random
from google.colab.patches import cv2_imshow

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

"""# Create dataset in detectron2 format"""

# expected folder structure:
# CellData
#     |- train 
#       |- images
#       |- segs
#     |- valid 
#       |- images
#       |- segs

# go to folder containing cell dataset
import os
root = "/content/drive/MyDrive/data/summer-project/"
os.chdir(root)
!pwd
# %cd root

# # zip celldata
# !tar -zcvf CellData.tar.gz CellData/ 
# !ls

dataset_path = root + "CellData/"
train_data = "train/images/"
val_data = "valid/images/"

import cv2
from google.colab.patches import cv2_imshow
import numpy as np
import glob

FILE_EXTENSION = "/*.tif"
TRAIN_IMAGE_PATH = dataset_path + train_data

VAL_IMAGE_PATH = dataset_path + val_data

import scipy
print(scipy.version.version)
from scipy import ndimage

'''
Create separate mask images list from a mask file.
The detectron2 dataset requires a mask dictionary for each instance, but
a mask file can have multiple masks.
ref: https://stackoverflow.com/questions/57192138/how-to-create-separate-images-containing-one-instance-mask-per-image-from-a-sing
param filename: Path to image which has corresponding mask image
returns: list of separate mask images which are 
of same size and only have 1 mask each. 
each image is a numpy array of dtype int64.
'''
def create_separate_masks(filename):
    # get matching mask file path
    mask_path = filename.replace("images", "segs")
    # get mask image
    mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)
    masks_list = []

    # get masks labelled with different values
    label_im, nb_labels = scipy.ndimage.label(mask) 

    for i in range(nb_labels):

        # create an array which size is same as the mask but filled with 
        # values that we get from the label_im. 
        # If there are three masks, then the pixels are labeled 
        # as 1, 2 and 3.

        mask_compare = np.full(np.shape(label_im), i+1) 

        # check equality test and have the value 1 on the location of each mask
        separate_mask = np.equal(label_im, mask_compare).astype(int) 

        # replace 1 with 255 for visualization as rgb image

        separate_mask[separate_mask == 1] = 255 

        masks_list.append(separate_mask)
    return masks_list

# use pycocotools to do mask encoding
import pycocotools
import pycocotools._mask as _mask

'''
Create an image's segmentation mask dictionary to
match detectron2 custom format. ref:
https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html
param img: mask image containing only 1 instance
returns: per-pixel segmentation mask in COCO's compressed RLE format
'''
def get_mask_as_dict(img):
  # convert grayscale image to have one more channel since mask encoding
  # needs 3 dimensional image, ex. (773, 739, 1)
  height, width = img.shape
  img_reshaped = img.reshape(height, width, 1)
  # read the mask image and convert to image with only 0's and 1's.
  # Surrounding area is 0, mask is 1
  binary_mask = np.where(img_reshaped > 0, 1, 0)
  # cast binary mask int64 to uint8
  binary_mask = binary_mask.astype(np.uint8)
  # convert a uint8 segmentation mask of 0s and 1s into dict by
  # using pycocotools
  mask_as_dict = pycocotools._mask.encode(np.asarray(binary_mask, order="F"))
  return mask_as_dict

import json
import numpy as np
from pycocotools import mask
from skimage import measure
'''
Create an image's segmentation mask as polygon list to
match detectron2 custom format.
ref: https://github.com/cocodataset/cocoapi/issues/131
param img: mask image containing only 1 instance
returns: list[list[float]], 
it represents a list of polygons, one for each connected component of the object.
'''
def get_mask_as_polygon_list(img):
  segmentation_list = []

  height, width = img.shape
  binary_mask = np.where(img > 0, 1, 0)
  # cast binary mask int64 to uint8
  binary_mask = binary_mask.astype(np.uint8)
  fortran_ground_truth_binary_mask = np.asfortranarray(binary_mask)
  contours = measure.find_contours(binary_mask, 0.5)

  for contour in contours:
      contour = np.flip(contour, axis=1)
      segmentation = contour.ravel().tolist()
      segmentation_list.append(segmentation)

  return segmentation_list

'''
Get bounding box from a compressed mask image dictionary
param compressed_mask: per-pixel segmentation mask in COCO's compressed RLE format
returns: one box in format [xmin, ymin, xmax, ymax]
'''
def get_box_from_compressed(compressed_mask):
  # bounding boxes, which is a list of boxes. each box is coordinate list.
  # Since there is 1 mask, there should only be 1 box in COCO format.
  # [top left x position, top left y position, width, height]
  # example boxes: [[665.  35.  60.  54.]]
  boxes = pycocotools._mask.toBbox(compressed_mask)
  assert len(boxes) == 1
  box = boxes[0]

  xmin = int(box[0])
  ymin = int(box[1])
  xmax = int(xmin + box[2])
  ymax = int(ymin + box[3])
  return [xmin, ymin, xmax, ymax]

from detectron2.structures import BoxMode
'''
Create annotations list of dictionaries required by
detectron2 instance segmentation dataset format.
param masks_list: list of individual mask images, with 1 mask per image.
returns: list of dictionaries, where each dictionary represents 1 instance
and contains the keys: 
  bbox (list[float], required)
  bbox_mode (int, required)
  category_id (int, required)
  segmentation (dict)
'''
def get_annotations(masks_list):
  objs = []
  for idx, mask in enumerate(masks_list):
      # only used to get bounding boxes
      seg_dict = get_mask_as_dict(mask)
      box = get_box_from_compressed(seg_dict)

      seg_poly_list = get_mask_as_polygon_list(mask)

      # category_id = 0 since background is 1
      obj = {
      "bbox": box,
      "bbox_mode": BoxMode.XYXY_ABS,
      # "segmentation": seg_dict,
      "segmentation": seg_poly_list,
      "category_id": 0,
      }
      objs.append(obj)
  return objs

'''
Create list of dictionaries which is the standard dataset representation
required by detectron2.
param img_dir: The train or validation directory path
returns: list of dictionaries, with each dict containing
filename, image ID, height, width, annotations
'''
def create_cell_dicts(img_dir):
    dataset_dicts = []
    for idx, file in enumerate(sorted(glob.glob(img_dir + FILE_EXTENSION))): 
        record = {}
        
        filename = os.path.join(img_dir, file)
        height, width = cv2.imread(filename).shape[:2]
        
        record["file_name"] = filename
        # image id must be unique
        record["image_id"] = idx
        record["height"] = height
        record["width"] = width

        # create separate mask images
        masks = create_separate_masks(filename)

        record["annotations"] = get_annotations(masks)
      
        dataset_dicts.append(record)

        # stop at 5 records if only testing
        # if idx == 4:
        #   break
    return dataset_dicts

for dir in [TRAIN_IMAGE_PATH, VAL_IMAGE_PATH]:
  print("creating dataset list dicts from:")
  print(dir)

  # d is postfix to registered dataset name
  # register cell_train and cell_valid datasets
  if (dir == TRAIN_IMAGE_PATH):
    d = "train"
  elif (dir == VAL_IMAGE_PATH):
    d = "val"
  else:
    sys.exit("directory path invalid")
  # register cell_train and cell_valid datasets
  DatasetCatalog.register("cell_" + d, lambda d=d: create_cell_dicts(dir))
  MetadataCatalog.get("cell_" + d).set(thing_classes=["cell"])
  
cell_metadata = MetadataCatalog.get("cell_train")

"""To verify the data loading is correct, visualize the annotations of randomly selected samples in the training set:


"""

dataset_dicts = create_cell_dicts(TRAIN_IMAGE_PATH)
for d in random.sample(dataset_dicts, 3):
    img = cv2.imread(d["file_name"], cv2.IMREAD_COLOR)
    visualizer = Visualizer(img[:, :, ::-1], metadata=cell_metadata, scale=0.5)
    out = visualizer.draw_dataset_dict(d)
    cv2_imshow(out.get_image()[:, :, ::-1])
    print("\n")

"""## Train

"""

from detectron2.engine import DefaultTrainer

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("cell_train",)
cfg.DATASETS.TEST = ()
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR
cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset
cfg.SOLVER.STEPS = []        # do not decay learning rate
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (cell). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)
# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg) 
trainer.resume_or_load(resume=False)
trainer.train()

# Commented out IPython magic to ensure Python compatibility.
# Look at training curves in tensorboard:
# %load_ext tensorboard
# %tensorboard --logdir output

"""## Inference & evaluation using the trained model
create a predictor using the trained model:


"""

# Inference should use the config with parameters that are used in training
# cfg now already contains everything we've set previously. We changed it a little bit for inference:
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")  # path to the model we just trained
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold
predictor = DefaultPredictor(cfg)

"""Randomly select several samples to visualize the prediction results."""

from detectron2.utils.visualizer import ColorMode
dataset_dicts = create_cell_dicts(VAL_IMAGE_PATH)
for d in random.sample(dataset_dicts, 3):    
    im = cv2.imread(d["file_name"], cv2.IMREAD_COLOR)
    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format
    v = Visualizer(im[:, :, ::-1],
                   metadata=cell_metadata, 
                   scale=0.5, 
                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models
    )

    truth_out = visualizer.draw_dataset_dict(d)
    cv2_imshow(truth_out.get_image()[:, :, ::-1])
    print("\n")

    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2_imshow(out.get_image()[:, :, ::-1] * 255)
    print("\n")

"""Evaluate model performance using AP metric implemented in COCO API."""

from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader
evaluator = COCOEvaluator("cell_val", ("bbox", "segm"), False, output_dir="./output/")
val_loader = build_detection_test_loader(cfg, "cell_val")
print(inference_on_dataset(trainer.model, val_loader, evaluator))
# another equivalent way to evaluate the model is to use `trainer.test`